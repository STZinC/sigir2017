{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from functions import *\n",
    "from twitterTokenizer import Tokenizer\n",
    "import numpy as np, random\n",
    "import subprocess \n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from keras.layers.normalization  import BatchNormalization\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, Bidirectional, LSTM, Input, merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train = load_SemEval_from_file('./data/subtaskCE.train_dev.tsv')\n",
    "X_dev, y_dev = load_SemEval_from_file('./data/subtaskCE.devtest.tsv')\n",
    "X_test, y_test = load_SemEval_SubTaskCE_Test('./data/SemEval2016-task4-test.subtask-BCDE.txt', './data/SemEval2016_task4_subtaskC_test_gold.txt')\n",
    "X_train_ternary, y_train_ternary = load_SemEval_subtaskA('./data/subtaskA.downloaded.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_additional = load_sparse_csr('./additional_features/X_train_additional.npz', )\n",
    "X_dev_additional = load_sparse_csr('./additional_features/X_dev_additional.npz', )\n",
    "X_test_additional = load_sparse_csr('./additional_features/X_test_additional.npz',)\n",
    "X_ternary_additional = load_sparse_csr('./additional_features/X_ternary_additional.npz',)\n",
    "X_train_additional.shape, X_dev_additional.shape,  X_test_additional.shape, X_ternary_additional.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train shape', (7292, 14356), 'Dev shape', (1778, 14356), 'Test shape', (20632, 14356), '14356 vocabulary terms found')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "MAX_FEATURES, MAX_LEN, BATCH_SIZE  = 11000, 30, 64\n",
    "\n",
    "tokenizer = Tokenizer(preserve_case=False)\n",
    "\n",
    "vec = CountVectorizer( ngram_range=(1,1), analyzer='word', tokenizer=tokenizer.tokenize, stop_words=None)\n",
    "vec.fit(X_train+X_train_ternary)\n",
    "\n",
    "x_train = vec.transform(X_train)\n",
    "x_train_ternary = vec.transform(X_train_ternary)\n",
    "x_dev = vec.transform(X_dev)\n",
    "x_test = vec.transform(X_test)\n",
    "\n",
    "print(\"Train shape\", x_train.shape, \"Dev shape\", x_dev.shape, \"Test shape\", x_test.shape, \"%d vocabulary terms found\"%len(vec.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_nn = np.split(x_train.indices, x_train.indptr[1:-1])\n",
    "x_train_ternary_nn = np.split(x_train_ternary.indices, x_train_ternary.indptr[1:-1])\n",
    "x_dev_nn = np.split(x_dev.indices, x_dev.indptr[1:-1])\n",
    "x_test_nn = np.split(x_test.indices, x_test.indptr[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "('X_train shape:', (7292, 30))\n",
      "('X_ternary shape:', (5500, 30))\n",
      "('X_dev shape:', (1778, 30))\n",
      "('X_test shape:', (20632, 30))\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train_nn = sequence.pad_sequences(x_train_nn, maxlen=MAX_LEN)\n",
    "x_train_ternary_nn = sequence.pad_sequences(x_train_ternary_nn, maxlen=MAX_LEN)\n",
    "x_dev_nn = sequence.pad_sequences(x_dev_nn, maxlen=MAX_LEN)\n",
    "x_test_nn = sequence.pad_sequences(x_test_nn, maxlen=MAX_LEN)\n",
    "print('X_train shape:', x_train_nn.shape)\n",
    "print('X_ternary shape:', x_train_ternary_nn.shape)\n",
    "print('X_dev shape:', x_dev_nn.shape)\n",
    "print('X_test shape:', x_test_nn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(\"./data/\", 'glove.twitter.27B.50d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "embedding_matrix = np.zeros((len(vec.vocabulary_) + 1, EMBEDDING_DIM))\n",
    "for key,val in vec.vocabulary_.iteritems():\n",
    "    embedding_vector = embeddings_index.get(key)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[val] = embedding_vector\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(classes=[-2, -1, 0 , 1, 2])\n",
    "y_train_nn = mlb.fit_transform([[y] for y in y_train])\n",
    "y_test_nn = mlb.transform([[y] for y in y_test])\n",
    "\n",
    "mlb2 = MultiLabelBinarizer(classes=[-1, 0, 1])\n",
    "y_train_nn_ternary = mlb2.fit_transform([[y] for y in y_train_ternary])\n",
    "# y_test_nn = mlb.transform([[y] for y in y_train_task2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import utils \n",
    "class_weights = utils.compute_class_weight('balanced', [-2, -1, 0, 1, 2], y_train)\n",
    "class_weights= {class_id:class_weight for class_id, class_weight in zip(range(5), class_weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build models...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 30, 50)        717850      main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "aux_input (InputLayer)           (None, 1368)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_3 (BatchNorma (None, 30, 50)        200         embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 256)           350464      aux_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional)  (None, 100)           40400       batchnormalization_3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 256)           0           dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "merge_3 (Merge)                  (None, 356)           0           bidirectional_3[0][0]            \n",
      "                                                                   dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 30)            10710       merge_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 30)            0           dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "main_output (Dense)              (None, 5)             155         dropout_6[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1,119,779\n",
      "Trainable params: 1,119,679\n",
      "Non-trainable params: 100\n",
      "____________________________________________________________________________________________________\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 30, 50)        717850      main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "aux_input (InputLayer)           (None, 1368)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_3 (BatchNorma (None, 30, 50)        200         embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 256)           350464      aux_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional)  (None, 100)           40400       batchnormalization_3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 256)           0           dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "merge_3 (Merge)                  (None, 356)           0           bidirectional_3[0][0]            \n",
      "                                                                   dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 30)            10710       merge_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 30)            0           dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "aux_output (Dense)               (None, 3)             93          dropout_6[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1,119,717\n",
      "Trainable params: 1,119,617\n",
      "Non-trainable params: 100\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Build models...')\n",
    "\n",
    "\n",
    "main_input = Input(shape=(MAX_LEN,), dtype='int32', name='main_input')\n",
    "\n",
    "x = Embedding(input_dim = len(vec.vocabulary_)+1, output_dim = EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_LEN, trainable=True, dropout=0.3)(main_input)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "lstm_out = Bidirectional(LSTM(output_dim = 50, input_dim = EMBEDDING_DIM, dropout_W=0.3, dropout_U=0.3) )(x)\n",
    "\n",
    "\n",
    "auxiliary_input = Input(shape=(1368,), name='aux_input')\n",
    "t_auxiliary_input = Dense(256, activation='tanh')(auxiliary_input)\n",
    "t_auxiliary_input = Dropout(0.5)(t_auxiliary_input)\n",
    "\n",
    "x = merge([lstm_out, t_auxiliary_input], mode='concat')\n",
    "\n",
    "\n",
    "x = Dense(30, activation='tanh', )(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "task1_output = Dense(5, activation='softmax', name='main_output')(x)\n",
    "task2_output = Dense(3, activation='softmax', name='aux_output')(x)\n",
    "\n",
    "\n",
    "model_task1 = Model(input=[main_input, auxiliary_input], output=[task1_output])\n",
    "model_task2 = Model(input=[main_input, auxiliary_input], output=[task2_output])\n",
    "\n",
    "model_task1.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_task2.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_task1.summary()\n",
    "model_task2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1 , DEV: 2.00022992311 TEST: 1.99759518922\n",
      "Iteration: 2 , DEV: 1.08940587545 TEST: 0.944375122881\n",
      "Iteration: 3 , DEV: 1.06662913655 TEST: 0.901236294888\n",
      "Iteration: 4 , DEV: 0.892888542316 TEST: 0.749903717341\n",
      "Iteration: 5 , DEV: 1.01769313065 TEST: 0.889997392043\n",
      "Iteration: 6 , DEV: 0.93274052353 TEST: 0.783120843809\n",
      "Iteration: 7 , DEV: 0.8202430834 TEST: 0.703477192513\n",
      "Iteration: 8 , DEV: 0.801584122262 TEST: 0.685664678568\n",
      "Iteration: 9 , DEV: 0.886038024633 TEST: 0.738257428094\n",
      "Iteration: 10 , DEV: 0.775721125264 TEST: 0.707678836853\n",
      "Iteration: 11 , DEV: 0.812286828848 TEST: 0.709359320468\n",
      "Iteration: 12 , DEV: 0.777229307781 TEST: 0.673799638016\n",
      "Iteration: 13 , DEV: 0.849629140949 TEST: 0.691304221478\n",
      "Iteration: 14 , DEV: 0.850662295789 TEST: 0.742849976033\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "results = []\n",
    "for batch in range(600*10):\n",
    "    if random.random() < 0.8:\n",
    "        sample = np.random.randint(0, len(x_train_nn), BATCH_SIZE)\n",
    "        x_sampled, y_sampled, x_aux = x_train_nn[sample], y_train_nn[sample], X_train_additional[sample]\n",
    "        model_task1.train_on_batch({'main_input': x_sampled, 'aux_input': x_aux.todense() }, [y_sampled], class_weight=class_weights, sample_weight=None)\n",
    "    else:\n",
    "        sample = np.random.randint(0, len(x_train_ternary_nn), BATCH_SIZE)\n",
    "        x_sampled, y_sampled, x_aux = x_train_ternary_nn[sample], y_train_nn_ternary[sample], X_ternary_additional[sample]\n",
    "        model_task2.train_on_batch({'main_input': x_sampled, 'aux_input': x_aux.todense()}, [y_sampled], class_weight=None, sample_weight=None)\n",
    "        \n",
    "    if batch%57==0:\n",
    "        dev_preds = np.argmax(model_task1.predict({'main_input': x_dev_nn,'aux_input': X_dev_additional.todense() }, batch_size=BATCH_SIZE, verbose=0), axis=1)\n",
    "        test_preds = np.argmax(model_task1.predict({'main_input': x_test_nn, 'aux_input': X_test_additional.todense()}, batch_size=BATCH_SIZE, verbose=0), axis=1)\n",
    "        results.append([macroMAE(y_dev, dev_preds-2), macroMAE(y_test, test_preds-2)])\n",
    "        print \"Iteration:\", int(batch/57)+1, \"\\tDEV:\", results[-1][0], \"\\tTEST:\", results[-1][1]\n",
    "        \n",
    "#68 0.775022887016 0.778202313573 <- without extra features best\n",
    "#probably need to do some cross-val or increase the size of the validation set.. Increased dropout, helped. !Success!!!\n",
    "best_run = np.argmin(np.asarray(results)[:,0])\n",
    "print results[best_run]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
